{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Lambda, TimeDistributed, Dense, BatchNormalization, Merge, Concatenate\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_dir=os.getcwd()\n",
    "data_dir=os.path.join(cur_dir, 'data')\n",
    "save_dir=os.path.join(data_dir, 'save')\n",
    "model_dir=os.path.join(os.path.dirname(cur_dir), 'models')\n",
    "glove_file=os.path.join(model_dir, 'glove.6B/glove.6B.300d.txt')\n",
    "if not os.path.exists(save_dir): os.mkdir(save_dir)\n",
    "    \n",
    "max_nb_words=200000\n",
    "max_seq_length=25\n",
    "embedding_dim=300\n",
    "val_split=0.1\n",
    "test_split=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "df['question1']=df['question1'].apply(str)\n",
    "df['question2']=df['question2'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions=list(df['question1'])+list(df['question2'])\n",
    "tokenizer=Tokenizer(max_nb_words)\n",
    "tokenizer.fit_on_texts(questions)\n",
    "q1_seqs=tokenizer.texts_to_sequences(list(df['question1']))\n",
    "q2_seqs=tokenizer.texts_to_sequences(list(df['question2']))\n",
    "word_index=tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_words=min(max_nb_words, len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:37, 10732.98it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_index={}\n",
    "f=open(glove_file, encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values=line.split(' ')\n",
    "    word=values[0]\n",
    "    coefs=np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word]=coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 35260\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix=np.zeros((nb_words+1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i>max_nb_words:\n",
    "        continue\n",
    "    embedding_vector=embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]=embedding_vector\n",
    "        \n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(embedding_matrix, open('glovemat6B.300d.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 29276\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix=pickle.load(open('glovemat840B.300d.pickle', 'rb'))\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 shape:  (404290, 25)\n",
      "Q2 shape:  (404290, 25)\n",
      "Labels shape:  (404290,)\n"
     ]
    }
   ],
   "source": [
    "q1_data=pad_sequences(q1_seqs, maxlen=max_seq_length)\n",
    "q2_data=pad_sequences(q2_seqs, maxlen=max_seq_length)\n",
    "labels=np.asarray(df['is_duplicate'], dtype=int)\n",
    "print('Q1 shape: ', q1_data.shape)\n",
    "print('Q2 shape: ', q2_data.shape)\n",
    "print('Labels shape: ', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=np.stack((q1_data, q2_data), axis=1)\n",
    "y=labels\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=val_split, random_state=13)\n",
    "Q1_train=X_train[:,0]\n",
    "Q2_train=X_train[:,1]\n",
    "Q1_test=X_test[:,0]\n",
    "Q2_test=X_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q1=Sequential()\n",
    "Q1.add(Embedding(nb_words+1, \n",
    "                 embedding_dim, \n",
    "                 weights=[embedding_matrix], \n",
    "                 input_length=max_seq_length, \n",
    "                 trainable=False))\n",
    "Q1.add(TimeDistributed(Dense(embedding_dim, activation='relu')))\n",
    "Q1.add(Lambda(lambda x: K.max(x, axis=1), output_shape=(embedding_dim, )))\n",
    "\n",
    "Q2=Sequential()\n",
    "Q2.add(Embedding(nb_words+1, \n",
    "                 embedding_dim, \n",
    "                 weights=[embedding_matrix], \n",
    "                 input_length=max_seq_length, \n",
    "                 trainable=False))\n",
    "Q2.add(TimeDistributed(Dense(embedding_dim, activation='relu')))\n",
    "Q2.add(Lambda(lambda x: K.max(x, axis=1), output_shape=(embedding_dim, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel/__main__.py:2: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Merge([Q1, Q2], mode='concat'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(200,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(200,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(200,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_1 (Merge)              (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               120200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 57,744,401.0\n",
      "Trainable params: 383,801.0\n",
      "Non-trainable params: 57,360,600.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss='binary_crossentropy'\n",
    "optimizer='adam'\n",
    "metrics=['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"W4-{'val_loss': [0.37383292400570683], 'val_acc': [0.82675802024921841], 'loss': [0.32894619806340158], 'acc': [0.85018729679789484]}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit([q1_data, q2_data], labels, epochs=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft=pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "dft['question1']=dft['question1'].apply(str)\n",
    "dft['question2']=dft['question2'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_id=dft['test_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tq1_seqs=tokenizer.texts_to_sequences(list(dft['question1']))\n",
    "tq2_seqs=tokenizer.texts_to_sequences(list(dft['question2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tq1_data=pad_sequences(tq1_seqs, maxlen=max_seq_length)\n",
    "tq2_data=pad_sequences(tq2_seqs, maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 32/100 [========>.....................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "preds=model.predict_classes([tq1_data, tq2_data])\n",
    "preds=np.apply_along_axis(int, -1, preds)\n",
    "out_df = pd.DataFrame({\"test_id\":test_id[:100], \"is_duplicate\":preds})\n",
    "out_df.to_csv(\"s1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.apply_along_axis(int, -1, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
